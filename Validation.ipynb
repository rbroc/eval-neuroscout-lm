{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (GPT2LMHeadModel, GPT2TokenizerFast,\n",
    "                          BertForMaskedLM, BertTokenizerFast,\n",
    "                          DistilBertForMaskedLM, DistilBertTokenizerFast,\n",
    "                          RobertaForMaskedLM, RobertaTokenizerFast,\n",
    "                          BlenderbotForCausalLM, BlenderbotTokenizer,\n",
    "                          BigBirdForMaskedLM, BigBirdTokenizer,\n",
    "                          ElectraForMaskedLM, ElectraTokenizerFast,\n",
    "                          CTRLLMHeadModel, CTRLTokenizer)\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import glob\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmeval.datasets import NarrativesDataset\n",
    "from lmeval.engine import StridingLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize list of models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = glob.glob('inputs/narratives/gentle/*/transcript*')\n",
    "aligned = glob.glob('inputs/narratives/gentle/*/align.csv')\n",
    "dataset_files = transcripts + aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [GPT2LMHeadModel,\n",
    "                 BertForMaskedLM,\n",
    "                 DistilBertForMaskedLM, \n",
    "                 RobertaForMaskedLM, \n",
    "                 BlenderbotForCausalLM, \n",
    "                 BigBirdForMaskedLM,\n",
    "                 ElectraForMaskedLM]\n",
    "model_ids = ['gpt2', 'bert-base-uncased', 'distilbert-base-uncased',\n",
    "             'roberta-base', 'facebook/blenderbot-400M-distill', \n",
    "             'google/bigbird-roberta-base', 'google/electra-base-discriminator']\n",
    "tokenizer_classes = [GPT2TokenizerFast,\n",
    "                     BertTokenizerFast,\n",
    "                     DistilBertTokenizerFast,\n",
    "                     RobertaTokenizerFast,\n",
    "                     BlenderbotTokenizer,\n",
    "                     BigBirdTokenizer,\n",
    "                     ElectraTokenizerFast]\n",
    "model_parameters = list(zip(model_classes, model_ids, tokenizer_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_lengths = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all combinations of files, model_parameters, and context lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(product(dataset_files, \n",
    "                          model_parameters, \n",
    "                          ctx_lengths))\n",
    "parameters = [(i[0], *i[1], i[2]) for i in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation function + utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_dataset_id(datafile):\n",
    "    ds_name_splits = datafile.split('/')\n",
    "    narrative = ds_name_splits[3]\n",
    "    ds_type = ds_name_splits[-1].split('.')[0]\n",
    "    ds_id = '_'.join([narrative, ds_type])\n",
    "    return ds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(datafile, model_class, model_id, tokenizer_class, ctx_length):\n",
    "    print(datafile, model_id, ctx_length)\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_id)\n",
    "    model = model_class.from_pretrained(model_id)\n",
    "    dataset_name = _make_dataset_id(datafile)\n",
    "    data = NarrativesDataset(datafile, dataset_name)\n",
    "    data.text = data.text[:100]\n",
    "    engine = StridingLM(context_length=ctx_length)\n",
    "    result = engine.run(data, tokenizer, model, model_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.starmap(_validate, parameters)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(results):\n",
    "    if idx == 0:\n",
    "        r_all = df\n",
    "    else:\n",
    "        r_all = pd.concat([r_all, df], ignore_index=True)\n",
    "r_all.to_csv('outs/validation_0909.txt', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
