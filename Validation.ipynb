{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (GPT2LMHeadModel, GPT2TokenizerFast,\n",
    "                          BertForMaskedLM, BertTokenizerFast,\n",
    "                          DistilBertForMaskedLM, DistilBertTokenizerFast,\n",
    "                          RobertaForMaskedLM, RobertaTokenizerFast,\n",
    "                          BlenderbotForCausalLM, BlenderbotTokenizer,\n",
    "                          BigBirdForMaskedLM, BigBirdTokenizer,\n",
    "                          ElectraForMaskedLM, ElectraTokenizerFast)\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import glob\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmeval.datasets import NarrativesDataset\n",
    "from lmeval.engine import StridingMLM, StridingForwardLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize list of models and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on the metrics:\n",
    "- Loss: quantifies distance from real predictions [NS]\n",
    "- Probability true: quantifies accuracy in terms of probability attributed to the predicted word (inverse is surprisal) [NS]\n",
    "- Probability top: reference for probability true - **should maybe replace with rank!**\n",
    "- Entropy: quantifies uncertainty of the model over next-word prediction [NS]\n",
    "- Top true: quantifies general model accuracy as \"is the top prediction the correct one\"?\n",
    "- nr unique words / vocabulary size: quantifies how \"varied\" model predictions are\n",
    "\n",
    "Need to define:\n",
    "- Random baseline\n",
    "\n",
    "Questions:\n",
    "- Which model is the most accurate, in terms of cross-entropy loss, probability true, and probability top?\n",
    "- Which window yield the best predictions?\n",
    "- What type of text yields the best predictions?\n",
    "\n",
    "Additional to dos:\n",
    "- Qualitative inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = glob.glob('inputs/narratives/gentle/*/transcript*')\n",
    "aligned = glob.glob('inputs/narratives/gentle/*/align.csv')\n",
    "dataset_files = transcripts + aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [GPT2LMHeadModel,\n",
    "                 BertForMaskedLM,\n",
    "                 DistilBertForMaskedLM, \n",
    "                 RobertaForMaskedLM, \n",
    "                 BlenderbotForCausalLM, \n",
    "                 BigBirdForMaskedLM,\n",
    "                 ElectraForMaskedLM]\n",
    "model_ids = ['gpt2', \n",
    "             'bert-base-uncased', \n",
    "             'distilbert-base-uncased',\n",
    "             'roberta-base', \n",
    "             'facebook/blenderbot-400M-distill', \n",
    "             'google/bigbird-roberta-base', \n",
    "             'google/electra-base-discriminator']\n",
    "tokenizer_classes = [GPT2TokenizerFast,\n",
    "                     BertTokenizerFast,\n",
    "                     DistilBertTokenizerFast,\n",
    "                     RobertaTokenizerFast,\n",
    "                     BlenderbotTokenizer,\n",
    "                     BigBirdTokenizer,\n",
    "                     ElectraTokenizerFast]\n",
    "model_parameters = list(zip(model_classes, model_ids, tokenizer_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_lengths = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all combinations of files, model_parameters, and context lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(product(dataset_files, \n",
    "                          model_parameters, \n",
    "                          ctx_lengths))\n",
    "parameters = [(i[0], *i[1], i[2]) for i in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation function + utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_dataset_id(datafile):\n",
    "    ds_name_splits = datafile.split('/')\n",
    "    narrative = ds_name_splits[3]\n",
    "    ds_type = ds_name_splits[-1].split('.')[0]\n",
    "    ds_id = '_'.join([narrative, ds_type])\n",
    "    return ds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(datafile, model_class, model_id, tokenizer_class, ctx_length):\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_id)\n",
    "    model = model_class.from_pretrained(model_id)\n",
    "    dataset_name = _make_dataset_id(datafile)\n",
    "    data = NarrativesDataset(datafile, dataset_name)\n",
    "    data.text = data.text[:100]\n",
    "    if any([b in model_id for b in ['bert','electra',\n",
    "                                    'bigbird']]):\n",
    "        engine = StridingMLM(context_length=ctx_length)\n",
    "    else:\n",
    "        engine = StridingForwardLM(context_length=ctx_length)\n",
    "    result = engine.run(data, tokenizer, model, model_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    _validate(*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.starmap(_validate, parameters)\n",
    "pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_test",
   "language": "python",
   "name": "dl_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
